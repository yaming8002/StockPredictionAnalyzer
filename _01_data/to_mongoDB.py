from datetime import datetime
import os
import pandas as pd
import numpy as np
import pymongo
from typing import List, Optional

from modules.config_loader import load_config
from modules.logger import setup_logger
from modules.process_mongo import get_mongo_client
from .stock_technical import calculate_bollinger_bands, calculate_ema, calculate_sma

config = load_config()
logger = setup_logger("../toMongo.log")
def _find_col(df, aliases):
    lower_map = {c.lower(): c for c in df.columns}
    for name in aliases:
        if name.lower() in lower_map:
            return lower_map[name.lower()]
    return None

def process_csv_files():
    """è™•ç† CSV æª”æ¡ˆï¼Œå¢åŠ æŠ€è¡“æŒ‡æ¨™ä¸¦å¯«å…¥ MongoDB æˆ–è¼¸å‡ºç‚º CSV"""
    original_data_folder = config.get("original_data_folder", "./data")
    output_data_folder   = config.get("output_data_folder", "./processed_data")
    stock_list_file      = config.get("stock_list_path", "./stock_List.csv")  # è‚¡ç¥¨åå–®è·¯å¾‘
    use_mongodb          = config.get("use_mongodb", False)

    os.makedirs(output_data_folder, exist_ok=True)

    db = None
    if use_mongodb:
        db = get_mongo_client()

    # è¼‰å…¥è‚¡ç¥¨åå–®
    if os.path.exists(stock_list_file):
        stock_list_df = pd.read_csv(stock_list_file)
    else:
        stock_list_df = pd.DataFrame(columns=["stock_id"])  # ä¿åº•ç”¨

    for file in os.listdir(original_data_folder):
        if not file.endswith(".csv"):
            continue

        file_path = os.path.join(original_data_folder, file)
        df = pd.read_csv(file_path)

        # ---------- 1) æ—¥æœŸè™•ç†ï¼šä¿®æ­£ tz è­¦å‘Šï¼ˆâœ… æ–°å¢ utc=True + è½‰å°åŒ—æ™‚å€ï¼‰ ----------
        date_col = _find_col(df, ["date", "Date", "DATE"])
        if date_col:
            # å…ˆè½‰ UTCï¼ˆawareï¼‰ï¼Œå†è½‰ Asia/Taipeiï¼Œæœ€å¾Œæ‹¿æ‰ tzï¼ˆnaiveï¼‰
            dt = pd.to_datetime(df[date_col], errors="coerce", utc=True)               # âœ…
            dt = dt.dt.tz_convert("Asia/Taipei").dt.tz_localize(None)                  # âœ…
            df[date_col] = dt
            # ç§»é™¤ç„¡æ•ˆæ—¥æœŸ
            df = df[~df[date_col].isna()]
            # è¨­æˆç´¢å¼•
            df = df.set_index(date_col)
        else:
            # æ²’æ‰¾åˆ°æ—¥æœŸæ¬„ä½ï¼Œå°±å˜—è©¦æŠŠ index è½‰æˆæ—¥æœŸ
            idx = pd.to_datetime(df.index, errors="coerce", utc=True)                  # âœ…
            # idx å¯èƒ½å…¨æ˜¯ NaTï¼›åƒ…ä¿ç•™æœ‰æ•ˆ
            mask = ~idx.isna()
            df = df.loc[mask].copy()
            if df.empty:
                logger.error(f"{file} ç¼ºå°‘å¯è§£æçš„æ—¥æœŸæ¬„ä½æˆ–è³‡æ–™ç‚ºç©ºï¼Œå·²è·³é")
                continue
            idx = idx[mask].tz_convert("Asia/Taipei").tz_localize(None)                # âœ…
            df.index = idx

        # åˆ°é€™è£¡ä¿è­‰æ˜¯ DatetimeIndexï¼ˆå¦‚æœä¸æ˜¯ï¼Œç›´æ¥è·³éé€™å€‹æª”ï¼‰
        if not isinstance(df.index, pd.DatetimeIndex) or df.empty:
            logger.error(f"{file} ç¼ºå°‘å¯è§£æçš„æ—¥æœŸæ¬„ä½æˆ–è³‡æ–™ç‚ºç©ºï¼Œå·²è·³é")
            continue

        # æ­£è¦åŒ–ç‚ºã€Œç´”æ—¥æœŸçš„åˆå¤œæ™‚é–“ã€ï¼Œé¿å…å¤¾å¸¶æ™‚åˆ†ç§’
        df.index = df.index.normalize()
        df.index.name = "date"

        # ---------- 2) å»é‡ï¼šåŒæ—¥åªç•™æœ€å¾Œä¸€ç­† ----------
        before_len = len(df)
        df = df[~df.index.duplicated(keep="last")]
        after_len = len(df)
        dup_removed = before_len - after_len

        # ---------- 3) æ•¸å€¼æ¸…ç†ï¼ˆâœ… æ–°å¢ï¼‰ ----------
        # åµæ¸¬å¸¸è¦‹æ•¸å€¼æ¬„ä½ä¸¦è½‰ç‚º numeric
        price_cols = {
            "open":      _find_col(df, ["open", "opening", "o"]),
            "high":      _find_col(df, ["high", "h"]),
            "low":       _find_col(df, ["low", "l"]),
            "close":     _find_col(df, ["close", "c", "price"]),
            "adj_close": _find_col(df, ["adj close", "adj_close", "adjusted close", "adjusted_close", "ac"]),
        }
        vol_col = _find_col(df, ["volume", "vol", "volumn", "shares"])

        numeric_targets = [c for c in price_cols.values() if c] + ([vol_col] if vol_col else [])
        for col in numeric_targets:
            df[col] = pd.to_numeric(df[col], errors="coerce")  # éæ•¸å­— â†’ NaN

        # è¦å‰‡ï¼š
        # - ä»»ä¸€åƒ¹æ ¼æ¬„ä½ï¼ˆå­˜åœ¨è€…ï¼‰ç‚º NaN æˆ– â‰¤ 0 â†’ ç•¥éè©²åˆ—
        # - æˆäº¤é‡ï¼ˆå­˜åœ¨è€…ï¼‰ç‚º NaN æˆ– < 0 â†’ ç•¥éè©²åˆ—ï¼ˆå…è¨± 0ï¼‰
        bad_mask = pd.Series(False, index=df.index)
        for key, col in price_cols.items():
            if col:
                bad_mask |= (~np.isfinite(df[col])) | (df[col] <= 0)
        if vol_col:
            bad_mask |= (~np.isfinite(df[vol_col])) | (df[vol_col] < 0)

        invalid_rows = int(bad_mask.sum())
        if invalid_rows > 0:
            df = df.loc[~bad_mask]
            logger.warning(f"{file} ç§»é™¤ {invalid_rows} ç­†æ•¸å€¼æœ‰å•é¡Œçš„åˆ—ï¼ˆåƒ¹æ ¼<=0/éæ•¸å­— æˆ– æˆäº¤é‡<0/éæ•¸å­—ï¼‰")  # âœ…

        if df.empty:
            logger.error(f"{file} æ¸…ç†å¾Œæ²’æœ‰æœ‰æ•ˆè³‡æ–™ï¼Œå·²è·³é")
            # å¾è‚¡ç¥¨åå–®ç§»é™¤
            stock_id = file.replace(".csv", "")
            if "stock_id" in stock_list_df.columns:
                stock_list_df = stock_list_df[stock_list_df["stock_id"] != stock_id]
            continue

        # è‹¥æœ‰åˆªé™¤ï¼ˆé‡è¤‡æˆ–å£æ•¸å€¼ï¼‰ï¼Œå›å­˜åŸå§‹ CSVï¼ˆå«ç´¢å¼• 'date'ï¼‰
        if dup_removed > 0 or invalid_rows > 0:
            df.to_csv(file_path, index=True)
            logger.info(f"{file} å·²æ›´æ–° CSV æª”æ¡ˆï¼ˆç§»é™¤é‡è¤‡ {dup_removed} ç­†ã€å£æ•¸å€¼ {invalid_rows} ç­†ï¼‰")

        # ---------- 4) 2025-09 è‡³å°‘ 18 ç­† ----------
        start_date = pd.Timestamp("2025-09-01")
        end_date   = pd.Timestamp("2025-09-30")
        df_sep2025 = df[(df.index >= start_date) & (df.index <= end_date)]
        if len(df_sep2025) < 18:
            stock_id = file.replace(".csv", "")
            if "stock_id" in stock_list_df.columns:
                stock_list_df = stock_list_df[stock_list_df["stock_id"] != stock_id]
                logger.warning(f"{stock_id} åœ¨ 2025-09 åªæœ‰ {len(df_sep2025)} ç­†ï¼Œå·²å¾è‚¡ç¥¨åå–®ç§»é™¤")
            continue

        # ---------- 5) è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ ----------
        df = calculate_working(df)

        collection_name = file.replace(".csv", "")

        # ---------- 6) å¯«å…¥ MongoDB æˆ–è¼¸å‡º CSV ----------
        if use_mongodb:
            collection = db[collection_name]
            records = df.reset_index(names="date").to_dict(orient="records")

            if not records:
                logger.warning(f"{file} æ²’æœ‰æœ‰æ•ˆè³‡æ–™ï¼Œç•¥é")
                stock_id = file.replace(".csv", "")
                if "stock_id" in stock_list_df.columns:
                    stock_list_df = stock_list_df[stock_list_df["stock_id"] != stock_id]
                    stock_list_df.to_csv(stock_list_file, index=False, encoding="utf-8-sig")
                    logger.warning(f"{stock_id} å·²å¾è‚¡ç¥¨åå–®ç§»é™¤ï¼Œæ›´æ–° {stock_list_file}")
                continue

            for record in records:
                # çµ±ä¸€æ—¥æœŸç‚º YYYY-MM-DD å­—ä¸²
                d = record.get("date")
                if isinstance(d, pd.Timestamp):
                    record["date"] = d.strftime("%Y-%m-%d")
                elif isinstance(d, datetime.datetime):
                    record["date"] = d.strftime("%Y-%m-%d")
                elif isinstance(d, datetime.date):
                    record["date"] = d.isoformat()
                else:
                    record["date"] = str(d)[:10]

                collection.update_one(
                    {"date": record["date"]},   # ä»¥å­—ä¸²æ—¥æœŸåš upsert key
                    {"$set": record},
                    upsert=True
                )

            logger.info(f"{file} å·²å¯«å…¥/æ›´æ–°è‡³ MongoDB (å…± {len(records)} ç­†)")
        else:
            output_path = os.path.join(output_data_folder, file)
            df.to_csv(output_path, index=True)  # index.name='date' æœƒå¯«å‡º
            logger.info(f"{file} å·²å­˜å…¥ {output_path}")

    # æœ€å¾Œæ›´æ–°è‚¡ç¥¨åå–®
    logger.warning(f"æ›´æ–° {stock_list_file}")
    stock_list_df.to_csv(stock_list_file, index=False, encoding="utf-8-sig")



def calculate_working(df):
    df = calculate_sma(df, window=5)
    df = calculate_sma(df, window=20)
    df = calculate_sma(df, window=50)
    df = calculate_sma(df, window=60)
    df = calculate_sma(df, window=120)
    df = calculate_sma(df, window=200)
    df = calculate_ema(df, span=5)
    df = calculate_ema(df, span=20)
    df = calculate_ema(df, span=50)
    df = calculate_ema(df, span=60)
    df = calculate_ema(df, span=120)
    df = calculate_ema(df, span=200)
    df = calculate_bollinger_bands(df)

    return df.iloc[200:]


def remove_mongoDB():
    """æ¸…ç©º MongoDB ä¸­çš„ stock_analysis é›†åˆ"""
    use_mongodb = config.get("use_mongodb", False)
    if use_mongodb:
        mongo_uri = config.get("mongo_uri", "mongodb://localhost:27017")
        client = pymongo.MongoClient(mongo_uri)
        db_name = config.get("mongo_db", "stock_db")
        client.drop_database(db_name)
        print(f"MongoDB æ•¸æ“šåº« {db_name} ä¸­çš„æ‰€æœ‰é›†åˆå…§å®¹å·²æ¸…ç©º")


def update_all_stock_indicators(last_days: int = 100):
    """
    ğŸ”„ é‡æ–°è¨ˆç®— MongoDB æ‰€æœ‰è‚¡ç¥¨çš„æŠ€è¡“æŒ‡æ¨™ï¼ˆåƒ…å¯«å…¥æœ€å¾Œ N å¤©ï¼‰
    - é è¨­ N = 100
    - æœƒå¤šæŠ“å– N + 200 å¤©çš„è³‡æ–™ç”¨æ–¼ SMA/EMA åˆå§‹åŒ–
    - é©åˆæ¯å¤©æ›´æ–°æœ€æ–°æŠ€è¡“æŒ‡æ¨™ï¼Œä¸éœ€é‡ç®—æ•´å€‹æ­·å²
    """
    use_mongodb = config.get("use_mongodb", False)
    if not use_mongodb:
        logger.error("âš ï¸ config.yaml æœªå•Ÿç”¨ use_mongodb=Trueï¼Œç„¡æ³•æ›´æ–°è³‡æ–™åº«")
        return

    db = get_mongo_client()
    all_collections = [col for col in db.list_collection_names() if "TW" in col]
    if not all_collections:
        logger.warning("âš ï¸ MongoDB ç„¡è‚¡ç¥¨è³‡æ–™")
        return

    from pymongo import UpdateOne

    logger.info(f"ğŸ”„ é–‹å§‹æ›´æ–°æŠ€è¡“æŒ‡æ¨™ï¼Œå…± {len(all_collections)} æª”è‚¡ç¥¨ï¼ˆåƒ…æœ€å¾Œ {last_days} å¤©ï¼‰")

    for stock_id in all_collections:
        collection = db[stock_id]

        # å¤šå–200å¤©ä½œç‚ºç·©è¡å€ï¼Œç¢ºä¿EMAç­‰æŒ‡æ¨™ä¸å¤±çœŸ
        limit_days = last_days + 200
        cursor = collection.find({}, {"_id": 0}).sort("date", -1).limit(limit_days)
        df = pd.DataFrame(list(cursor))

        if df.empty or "date" not in df.columns:
            logger.warning(f"{stock_id} ç„¡æœ‰æ•ˆè³‡æ–™ï¼Œç•¥é")
            continue

        # æ—¥æœŸè™•ç†
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df[df["date"].notna()].sort_values("date")
        df.set_index("date", inplace=True)

        # ç¢ºä¿ä¸»è¦æ¬„ä½å­˜åœ¨
        required_cols = {"open", "high", "low", "close", "volume"}
        if not required_cols.issubset(df.columns):
            logger.warning(f"{stock_id} ç¼ºå°‘ä¸»è¦æ¬„ä½ï¼Œç•¥é ({df.columns})")
            continue

        # é‡æ–°è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        df = calculate_working(df)

        # åƒ…å–æœ€å¾Œ N å¤©è³‡æ–™
        df_recent = df.tail(last_days).copy()

        # æ‰¹æ¬¡ upsert å› MongoDB
        df_reset = df_recent.reset_index()
        df_reset["date"] = df_reset["date"].dt.strftime("%Y-%m-%d")
        records = df_reset.to_dict(orient="records")

        bulk_ops = []
        for record in records:
            bulk_ops.append(
                UpdateOne(
                    {"date": record["date"]},
                    {"$set": record},
                    upsert=True
                )
            )

        if bulk_ops:
            result = collection.bulk_write(bulk_ops, ordered=False)
            logger.info(
                f"âœ… {stock_id} æŒ‡æ¨™å·²æ›´æ–° (å¯«å…¥ {len(records)} ç­†, matched={result.matched_count}, modified={result.modified_count}, upserted={len(result.upserted_ids)})"
            )

    logger.info("ğŸ¯ å…¨éƒ¨è‚¡ç¥¨æŒ‡æ¨™æ›´æ–°å®Œæˆï¼ˆåƒ…æœ€å¾Œ 100 å¤©ï¼‰")
