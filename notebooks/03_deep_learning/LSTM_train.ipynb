{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9ef50a5-442b-4369-a4de-00e619064a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "\n",
      ">>lst_labels_train\n",
      "Elapsed time: 0:00:00\n",
      "{'Up_3days': 0, 'Down_3days': 0, 'Stable_3days': 0, 'Other': 0}\n",
      "\n",
      "\n",
      ">>lst_labels_validation\n",
      "Elapsed time: 0:05:26.501796\n",
      "{'Up_3days': 0, 'Down_3days': 0, 'Stable_3days': 0, 'Other': 0}\n",
      "\n",
      "\n",
      ">>lst_labels_test\n",
      "Elapsed time: 0:00:00\n",
      "{'Up_3days': 0, 'Down_3days': 0, 'Stable_3days': 0, 'Other': 0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pymongo\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "class StockDataProcessor:\n",
    "    def __init__(self, mongodb_url='mongodb://localhost:27017', days=60, num_records=1000, isFlatten=False  ):\n",
    "        self.client = pymongo.MongoClient(mongodb_url)\n",
    "        self.db = self.client['cleanData']\n",
    "        self.days = days\n",
    "        self.chaeckdays = 0 # \n",
    "        self.num_records = num_records\n",
    "        self.column=[\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\",\"SMA5\",\"SMA10\",\"SMA20\",\"SMA60\",\"HighestHigh\",\"LowestLow\",\"RSV\",\"KDJ_K\",\"KDJ_D\",\"KDJ_J\",\"dif\",\"dea\",\"macd_hist\"]\n",
    "        self.leable = [\"Up_3days\",\"Down_3days\",\"Stable_3days\",\"Other\",\"Volume_5avg\",\"Volume_flag\"]\n",
    "        self.label_tables = ['lst_labels_train', 'lst_labels_validation', 'lst_labels_test']\n",
    "        self.data_label = {table: [x['label'] for x in self.db[table].find({}, {'label': 1})] for table in self.label_tables}\n",
    "        self.data_label_count = {table:0 for table in self.label_tables  }\n",
    "            \n",
    "\n",
    "    def get_data(self, input_data, afterday=0):\n",
    "        id, date = input_data.split('_')\n",
    "        \n",
    "        date_obj = pd.to_datetime(date)\n",
    "        modified_date = date_obj + pd.DateOffset(days=afterday)\n",
    "        modified_date_str = modified_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        df = pd.DataFrame(list(self.db[id].find({'Date': {'$lte':modified_date_str}}).sort('Date', pymongo.DESCENDING).limit(self.days+afterday)))\n",
    "        # df = pd.DataFrame(list(collection.find({'Date': {'$lte':date}})))\n",
    "        data = df.sort_values('Date').reset_index(drop=True)\n",
    "        data =  df[self.column]\n",
    "        \n",
    "        data_label = df[self.leable].iloc[-1]\n",
    "        return data,data_label\n",
    "\n",
    "    def normalization(self, stock_data, isFlatten):\n",
    "        stock_data = pd.DataFrame(stock_data)\n",
    "        stock_data['Volume'] = stock_data['Volume'].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "        stock_data['Volume'] = stock_data['Volume'].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "        stock_data['Volume'] = stock_data['Volume'].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "\n",
    "        columns_to_normalize = [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\",\"SMA5\",\"SMA10\",\"SMA20\",\"SMA60\",\"HighestHigh\",\"LowestLow\"]\n",
    "        scaler = StandardScaler()\n",
    "        stock_data[columns_to_normalize] = scaler.fit_transform(stock_data[columns_to_normalize])\n",
    "        stock_data[[\"RSV\",\"KDJ_K\",\"KDJ_D\",\"KDJ_J\"]] = stock_data[[\"RSV\",\"KDJ_K\",\"KDJ_D\",\"KDJ_J\"]]*0.01\n",
    "        \n",
    "\n",
    "        return stock_data.values.reshape(-1) if self.isFlatten else stock_data\n",
    "    \n",
    "    def process_data(self, label, count_local):\n",
    "        '''\n",
    "        輔助 process_total 重複執行的項目\n",
    "        '''\n",
    "        x_datas = []\n",
    "        y_datas = []\n",
    "        alldata_size = len(self.data_label[label])\n",
    "        while self.data_label_count[label] < alldata_size and len(y_datas) < self.num_records :\n",
    "            x_data, y_data = self.get_data(self.data_label[label][self.data_label_count[label]])\n",
    "            x_datas.append(self.normalization(x_data))\n",
    "            y_datas.append(y_data)\n",
    "            count_local += 1\n",
    "            self.data_label_count[label] += 1\n",
    "            \n",
    "        return x_datas, y_datas\n",
    "    \n",
    "    def process_total(self, label):\n",
    "        count_local = 0\n",
    "        x_datas = []\n",
    "        y_datas = []\n",
    "        x_datas , y_datas = self.process_data(label,count_local)\n",
    "            \n",
    "        if  count_local < self.num_records:\n",
    "            self.data_label_count[label]  = 0\n",
    "            random.shuffle(self.data_label[label])\n",
    "            x_datas2 , y_datas2 = self.process_data(label,count_local)\n",
    "            x_datas.extend(x_datas2)\n",
    "            y_datas.extend(y_datas2)\n",
    "            \n",
    "\n",
    "        return np.array(x_datas), np.array(y_datas)\n",
    "\n",
    "    \n",
    "s = StockDataProcessor(num_records=10)\n",
    "print(s.data_label['lst_labels_train'][:3])\n",
    "_,yy=s.get_data('2371.TW.csv_2013-05-20',7)\n",
    "\n",
    "yy.to_dict()\n",
    "def clean_label(s,name) :\n",
    "    check = {\"Up_3days\": 0, \"Down_3days\": 0, \"Stable_3days\": 0, \"Other\": 0}\n",
    "    train_list = s.data_label[name]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    not_nan_list = []\n",
    "    for x in train_list:\n",
    "        train, y = s.get_data(x)\n",
    "        if train.isna:\n",
    "            continue\n",
    "        dct_y = y.to_dict()\n",
    "        for key in check.keys() :\n",
    "            check[key] += dct_y[key]\n",
    "        not_nan_list.append(x)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 计算经过的时间\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # 打印结果\n",
    "    delta_time = datetime.timedelta(seconds=elapsed_time)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"Elapsed time: {}\".format(str(delta_time)))\n",
    "    print(check)\n",
    "    not_nan_list = list(set(not_nan_list))\n",
    "    collection = s.db[name]\n",
    "    collection.drop()\n",
    "    # 將列表中的每一個元素轉為字典並插入到MongoDB\n",
    "    for value in not_nan_list:\n",
    "        collection.insert_one({\"label\": value})\n",
    "\n",
    "s = StockDataProcessor(num_records=10)\n",
    "for name in ['lst_labels_train', 'lst_labels_validation', 'lst_labels_test']:\n",
    "    print(f\"\\n\\n>>{name}\")\n",
    "    clean_label(s,name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d59f55-6947-48d5-ad04-b9bc41267f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "print(gpus, cpus)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9f9b2-a111-4485-a7f3-eb84619f6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Activation,Input,Flatten,BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam,Adamax\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time \n",
    "import sys\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# 設置 GPU 可見性\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# three_high_df size is 95450\n",
    "# five_high_df size is 57607\n",
    "# one_low_df size is 1572160\n",
    "# normal_df size is 11277551\n",
    "\n",
    "# 條件終止\n",
    "class StopTraining(Callback):\n",
    "    def __init__(self, monitor='val_accuracy', value=0.7, consecutive_times=3):\n",
    "        super(StopTraining, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.consecutive_times = consecutive_times\n",
    "        self.count = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        \n",
    "        # Check for NaN loss\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None and np.isnan(loss):\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nLoss is NaN, training stopped.\")\n",
    "            return\n",
    "\n",
    "        print('check ')\n",
    "        if current is None:\n",
    "            return\n",
    "        if current > self.value:\n",
    "            self.count += 1\n",
    "            if self.count >= self.consecutive_times:\n",
    "                self.model.stop_training = True\n",
    "                print(f\"\\nReached {self.consecutive_times} consecutive times with {self.monitor} > {self.value}, training stopped.\")\n",
    "        # else:\n",
    "        #     self.count = 0\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, days=30, num_records=1000, num_epochs=20, labels=[], value=0.7):\n",
    "        self.days = days\n",
    "        self.num_records = num_records\n",
    "        self.num_epochs = num_epochs\n",
    "        self.trainData = StockDataProcessor_weight( days=days-1, num_records=num_records*3, begin='2011-01-01', end='2019-01-01', ischeckReset=True)\n",
    "        self.testData = StockDataProcessor_weight( days=days-1, num_records=num_records, begin='2019-01-01', end='2022-12-31', ischeckReset=False)\n",
    "        self.value_size = self.trainData.get_value_size()\n",
    "        self.model = None\n",
    "        self.num_steps_per_epoch = self.trainData.countNum\n",
    "        \n",
    "        self.count_reload = 0\n",
    "        self.num_epochs = num_epochs\n",
    "        self.analysis_result = {}\n",
    "        self.isTest = False\n",
    "        self.labels = labels\n",
    "        self.train_history = None\n",
    "        self.value_size = self.testData.get_value_size()\n",
    "        self.stop_training_callback = StopTraining(value=value)\n",
    "\n",
    "        \n",
    "    def get_data(self,isTrain=True):\n",
    "        if isTrain:\n",
    "            return self.trainData.get_uneven_date()\n",
    "        return self.testData.get_uneven_date()\n",
    "\t\t\n",
    "    \n",
    "    def batch_generator(self, num_records=None ):\n",
    "        while True:\n",
    "            yield self.get_data()\n",
    "                    \n",
    "    def batch_generator_test(self,num_records=None ):\n",
    "        return self.get_data(isTrain=False )    \n",
    "    \n",
    "    def create_keras_model(self,print_model=True ):\n",
    "        K.clear_session()\n",
    "        \n",
    "        input_num = 2 \n",
    "        while input_num < (self.days*self.value_size):\n",
    "            input_num = input_num* 2\n",
    "            \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Input(shape=(self.days* self.value_size)))\n",
    "    \n",
    "        self.model.add(Dense((input_num), activation=LeakyReLU(0.1)))\n",
    "        self.model.add(Dense((input_num/2), activation=LeakyReLU(0.1)))\n",
    "        # self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense((input_num/4), activation=LeakyReLU(0.1)))\n",
    "        self.model.add(Dropout(0.6))\n",
    "        self.model.add(Dense(128, activation=LeakyReLU(0.1)))\n",
    "        self.model.add(Dense(len(self.labels),activation='softmax'))\n",
    "        # self.model.add(Activation('softmax'))\n",
    "                       \n",
    "        if(print_model):\n",
    "            print(self.model.summary())\n",
    "            plot_model( self.model, to_file=\"model.png\", show_shapes=True, show_layer_names=True,expand_nested=True,)\n",
    "            img = plt.imread('model.png')\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "        # 创建 Adamax 优化器实例\n",
    "        optimizer = Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "        # 将优化器应用于模型编译\n",
    "        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  \n",
    "        # self.model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    " \n",
    "\n",
    "    def reshapeResult(self, x = []):\n",
    "        num = 0 \n",
    "        for i in range(len(x)):\n",
    "            num += x[i]*(2**i)\n",
    "        return num         \n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        # 建立Keras模型\n",
    "        self.create_keras_model()\n",
    "\n",
    "        self.train_data()\n",
    "        \n",
    "        self.model_test()\n",
    "\n",
    "    def train_data(self):\n",
    "\n",
    "        train_history = self.model.fit(\n",
    "            x=self.batch_generator(),\n",
    "            steps_per_epoch=self.num_steps_per_epoch,\n",
    "            epochs=self.num_epochs,\n",
    "            validation_data=self.batch_generator_test(),\n",
    "            # callbacks=[self.stop_training_callback]\n",
    "        )\n",
    "\n",
    "        print(f\"self.count_reload:{self.count_reload}\")\n",
    "        print(train_history.history.keys())\n",
    "\n",
    "        self.show_train_history(train_history, 'accuracy', 'val_accuracy')\n",
    "        self.show_train_history(train_history, 'loss', 'val_loss')\n",
    "\n",
    "        \n",
    "    def show_train_history(self, train_history,train,validation):\n",
    "\n",
    "      plt.plot(train_history.history[train])\n",
    "      plt.plot(train_history.history[validation])\n",
    "      plt.title('Train history')\n",
    "      plt.ylabel('train')\n",
    "      plt.xlabel('epoch')\n",
    "\n",
    "      # 設置圖例在左上角\n",
    "      plt.legend(['train','validation'],loc='upper left')\n",
    "      plt.show()       \n",
    "        \n",
    "    def model_test(self):\n",
    "        test_data,y_true = self.batch_generator_test()\n",
    "        loss ,acc = self.model.evaluate(test_data,y_true)\n",
    "        print(loss ,acc)\n",
    "        predictions = self.model.predict(test_data)\n",
    "        y_pred = (predictions > 0.5).astype(int)\n",
    "        y_pred = np.array( [self.reshapeResult(x) for x in y_pred ]).reshape(-1)\n",
    "        y_true =np.array( [self.reshapeResult(x) for x in y_true ]).reshape(-1)\n",
    "        m = confusion_matrix(y_true,y_pred)\n",
    "        print(f'confusion_matrix:\\n{m}')\n",
    "        self.analysis_result['confusion_matrix'] = m\n",
    "        \n",
    "        \n",
    "# test = DataGenerator(date_size=30,pick=2000,num_epochs=40)  \n",
    "# test = DataGenerator(date_size=60,pick=1000,num_epochs=40)     \n",
    "# three_high_normal = DataGenerator(days=120,num_records=2000,num_epochs=30,value=0.7, labels= ['three_high','normal'])\n",
    "three_high_normal = DataGenerator(days=60,num_records=2000,num_epochs=30,value=0.7, labels=  ['three_days_up','three_days_down','normal_up_down'])\n",
    "# three_high_normal.batch_generator_test()\n",
    "three_high_normal.build_model()\n",
    "# three_high_normal.model.save(f'./model/{datetime.now().strftime(\"%Y-%m-%d_\")}three_high_normal_90')\n",
    "# three_high_normal.model.save(f'./model/{datetime.now().strftime(\"%Y-%m-%d_\")}three_high_normal_120')\n",
    "\n",
    "# batch_gen = three_high_normal.batch_generator(num_records=10)  # 假設每批次生成 10 條記錄\n",
    "# 從生成器中提取指定數量的批次並打印內容\n",
    "# for i in range(3):\n",
    "#     print(f\"Batch {i + 1}:\")\n",
    "#     batch = next(batch_gen)\n",
    "#     print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
