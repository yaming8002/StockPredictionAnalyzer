# from random import random
# import numpy as np
# import pandas as pd
# import tensorflow as tf
# from sklearn.model_selection import train_test_split
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input, BatchNormalization
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.regularizers import l2
# from tensorflow.keras.callbacks import EarlyStopping
# import numpy as np
# import tensorflow as tf
# from sklearn.metrics import confusion_matrix, classification_report
# import matplotlib.pyplot as plt
# import seaborn as sns
# from _03_deeplearning.unit import build_training_data_gen, build_training_data_gen_from_df


# def build_model(input_shape, learning_rate=0.001, dropout_rate=0.5, conv_filters=64, dense_units=64):
#     """
#     å»ºç«‹ CNN æ¨¡å‹ï¼Œç”¨æ–¼äºŒå…ƒåˆ†é¡ï¼ˆè¼¸å‡ºç‚º sigmoidï¼‰

#     åƒæ•¸:
#         input_shape (tuple): è¼¸å…¥è³‡æ–™å½¢ç‹€ï¼Œä¾‹å¦‚ (30, 19)
#         learning_rate (float): å„ªåŒ–å™¨å­¸ç¿’ç‡
#         dropout_rate (float): Dropout æ¯”ä¾‹ï¼Œç”¨ä¾†é¿å… overfitting
#         conv_filters (int): Conv1D éæ¿¾å™¨æ•¸é‡
#         dense_units (int): Dense å±¤ç¥ç¶“å…ƒæ•¸é‡

#     å›å‚³:
#         model (tf.keras.Model): ç·¨è­¯å¥½çš„æ¨¡å‹
#     """
#     model = Sequential(
#         [
#             Input(shape=input_shape, name="Input_Layer"),
#             Conv1D(filters=conv_filters, kernel_size=3, activation="relu", kernel_regularizer=l2(0.001), name="Conv1D"),
#             BatchNormalization(name="BN_Conv"),
#             MaxPooling1D(pool_size=2, name="MaxPool"),
#             Flatten(name="Flatten"),
#             Dense(dense_units, activation="relu", kernel_regularizer=l2(0.001), name="Dense"),
#             BatchNormalization(name="BN_Dense"),
#             Dropout(dropout_rate, name="Dropout"),
#             Dense(1, activation="sigmoid", name="Output"),
#         ]
#     )

#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss="binary_crossentropy", metrics=["accuracy"])

#     return model


# def train(X, y, epochs=10, batch_size=32):
#     model = build_model(input_shape=(X.shape[1], X.shape[2]))
#     model.summary()
#     model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2)
#     model.save("model_stock_cnn.h5")


# def generator_wrapper(file_path):
#     return lambda: build_training_data_gen(file_path)


# def test_04_train_model(file_path="./stock_data/leaning_label/sma_120_sma_200_trades.csv", model_path="model_stream_cnn.h5"):
#     # ğŸ” ä¸€æ¬¡æŠŠæ‰€æœ‰è³‡æ–™å…ˆè®€å‡ºä¾†ï¼ˆéœæ…‹æ¨¡å¼ï¼‰
#     X_all, y_all = [], []
#     trades = pd.read_csv(file_path, parse_dates=["buy_date"])
#     trades = trades.sample(frac=1).reset_index(drop=True)
#     # âœ… åˆ‡åˆ† train / valï¼ˆ80% / 20%ï¼‰
#     split_index = int(len(trades) * 0.8)
#     train_trades = trades[:split_index]
#     val_trades = trades[split_index:]

#     # âœ… å¾è¨“ç·´è³‡æ–™ generator å–å¾— input_shape
#     sample_X, _ = next(build_training_data_gen_from_df(train_trades))
#     input_shape = sample_X.shape

#     # âœ… å»ºè¨“ç·´è³‡æ–™é›†
#     train_dataset = (
#         tf.data.Dataset.from_generator(
#             lambda: build_training_data_gen_from_df(train_trades),
#             output_signature=(
#                 tf.TensorSpec(shape=input_shape, dtype=tf.float32),
#                 tf.TensorSpec(shape=(), dtype=tf.int32),
#             ),
#         )
#         .batch(64)
#         .prefetch(tf.data.AUTOTUNE)
#     )

#     # âœ… é©—è­‰è³‡æ–™é›†
#     val_dataset = (
#         tf.data.Dataset.from_generator(
#             lambda: build_training_data_gen_from_df(val_trades),
#             output_signature=(
#                 tf.TensorSpec(shape=input_shape, dtype=tf.float32),
#                 tf.TensorSpec(shape=(), dtype=tf.int32),
#             ),
#         )
#         .batch(64)
#         .prefetch(tf.data.AUTOTUNE)
#     )

#     # âœ… å»ºæ¨¡å‹
#     model = build_model(input_shape=input_shape)

#     # âœ… class_weight è¨­å®š
#     win_trades = train_trades[train_trades["profit"] > 0]
#     win_per = len(win_trades) / len(train_trades) * 1.2
#     class_weight = {0: 1.0, 1: win_per}

#     # âœ… EarlyStopping
#     early_stop = EarlyStopping(patience=5, restore_best_weights=True)

#     # âœ… è¨“ç·´æ¨¡å‹
#     model.fit(train_dataset, validation_data=val_dataset, epochs=100, class_weight=class_weight, callbacks=[early_stop])

#     # âœ… å„²å­˜æ¨¡å‹
#     model.save(model_path)
#     print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å„²å­˜")

#     # âœ… æ¸¬è©¦è³‡æ–™éš¨æ©ŸæŠ½æ¨£
#     print("ğŸ¯ éš¨æ©ŸæŠ½æ¨£ 100 ç­†è³‡æ–™åšæ¸¬è©¦")
#     X_test, y_test = [], []
#     for X, y in build_training_data_gen_from_df(trades):  # ğŸ”„ ä½¿ç”¨åŒä¸€ä»½è³‡æ–™ä¾†æº
#         if random() < 0.1:
#             X_test.append(X)
#             y_test.append(y)
#         if len(X_test) >= 100:
#             break
#     X_test = np.array(X_test)
#     y_test = np.array(y_test)

#     # âœ… è©•ä¼°
#     loss, acc = model.evaluate(X_test, y_test)
#     print(f"ğŸ“Š æ¸¬è©¦é›†æº–ç¢ºç‡: {acc:.4f}, æå¤±: {loss:.4f}")
#     print(f"ğŸ“Š è¨“ç·´é›†ï¼šè³ºéŒ¢ç­†æ•¸ {len(train_trades[train_trades['profit'] > 0])} / ç¸½ç­†æ•¸ {len(train_trades)}")
#     print(f"ğŸ“Š æ¸¬è©¦é›†ï¼šè³ºéŒ¢ç­†æ•¸ {len(val_trades[val_trades['profit'] > 0])} / ç¸½ç­†æ•¸ {len(val_trades)}")


# def test_05_predict_from_model(file_path="./stock_data/leaning_label/sma_120_sma_200_trades.csv", model_path="model_stream_cnn.h5"):
#     # âœ… è¼‰å…¥æ¨¡å‹
#     print("ğŸ“¥ è¼‰å…¥æ¨¡å‹ä¸­...")
#     model = tf.keras.models.load_model(model_path)

#     # âœ… æ”¶é›†æ‰€æœ‰è³‡æ–™
#     print("ğŸ“¦ æº–å‚™æ¸¬è©¦è³‡æ–™ä¸­...")
#     X_test, y_test = [], []
#     count_line = 0
#     for X, y in build_training_data_gen(file_path):
#         X_test.append(X)
#         y_test.append(y)
#         count_line += 1
#         if count_line > 1000:
#             break

#     X_test = np.array(X_test)
#     y_test = np.array(y_test)

#     print(f"âœ… ç¸½è³‡æ–™æ•¸é‡ï¼š{len(X_test)}")

#     # âœ… æ¨¡å‹è©•ä¼°
#     loss, acc = model.evaluate(X_test, y_test)
#     print(f"ğŸ“Š æº–ç¢ºç‡: {acc:.4f}, æå¤±: {loss:.4f}")

#     # âœ… é æ¸¬èˆ‡è™•ç†çµæœ
#     y_prob = model.predict(X_test)
#     y_pred = (y_prob > 0.5).astype(int).reshape(-1)

#     # âœ… æ··æ·†çŸ©é™£
#     cm = confusion_matrix(y_test, y_pred, labels=[0, 1])

#     # åŒ…è£æˆ DataFrame åŠ ä¸Šæ¨™é¡Œ
#     cm_df = pd.DataFrame(cm, index=["å¯¦éš›ï¼šè³ éŒ¢ï¼ˆ0ï¼‰", "å¯¦éš›ï¼šè³ºéŒ¢ï¼ˆ1ï¼‰"], columns=["é æ¸¬ï¼šè³ éŒ¢ï¼ˆ0ï¼‰", "é æ¸¬ï¼šè³ºéŒ¢ï¼ˆ1ï¼‰"])

#     print("\nğŸ§¾ æ··æ·†çŸ©é™£ï¼ˆå«æ¬„ä½æ¨™ç±¤ï¼‰ï¼š")
#     print(cm_df)

#     # âœ… ç²¾ç¢ºç‡/å¬å›ç‡/F1
#     print("\nğŸ“ˆ åˆ†é¡è©•ä¼°å ±å‘Šï¼š")
#     print(classification_report(y_test, y_pred, digits=4))

#     # âœ… ç•«åœ–
#     plt.figure(figsize=(6, 5))
#     sns.heatmap(
#         cm,
#         annot=True,
#         fmt="d",
#         cmap="Blues",
#         xticklabels=["é æ¸¬ï¼šè³ ", "é æ¸¬ï¼šè³º"],
#         yticklabels=["å¯¦éš›ï¼šè³ ", "å¯¦éš›ï¼šè³º"],
#     )
#     plt.xlabel("é æ¸¬çµæœ")
#     plt.ylabel("å¯¦éš›æ¨™ç±¤")
#     plt.title("ğŸ¯ æ··æ·†çŸ©é™£")
#     plt.tight_layout()
#     plt.show()
